---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.15.0
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```{python}
import numpy as np
import pandas as pd
from glmnet import GLM, GLMNet, GaussNet
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold
import rpy2
# %load_ext rpy2.ipython

```

```{python}
n, p = 103, 20
rng = np.random.default_rng(0)
X = rng.standard_normal((n, p))
R = rng.standard_normal(n) 
O = rng.standard_normal(n) * 0.2
W = rng.uniform(0, 1, size=n)
W[:20] = 3
D = np.array([R, O, W]).T
Df = pd.DataFrame(D, columns=['response', 'offset', 'weight'])
```

# Check a GLM

```{python}
G1 = GLM(response_col=0,
        offset_col=1,
        weight_col=2)
G1.fit(X, D)
```

## Also using `pd.DataFrame`

```{python}
G2 = GLM(response_col='response',
        offset_col='offset',
        weight_col='weight')
G2.fit(X, Df)
assert np.allclose(G2.coef_, G1.coef_)
assert np.allclose(G2.intercept_, G1.intercept_)
```



```{r magic_args="-i R,X,O,W -o C"}
M = lm(R ~ X, weight=W, offset=O)
C = coef(M)
```

```{python}
assert np.allclose(G2.coef_, C[1:])
assert np.allclose(G2.intercept_, C[0])
```

## Try out dropping weights or offset

```{python}
G4 = GLM(response_col='response')
G4.fit(X, Df)
```

```{python}
G5 = GLM(response_col='response', weight_col='weight')
G5.fit(X, Df)
```

```{python}
G5 = GLM(response_col='response', offset_col='offset')
G5.fit(X, Df)
```

# Try GLMNet (family version)

```{python}
GN = GLMNet(response_col='response',
            offset_col='offset',
            weight_col='weight')
GN.fit(X, Df)
```

```{r magic_args="-o C"}
library(glmnet)
GN = glmnet(X, R, offset=O, weights=W)
GN$lambda
C = as.matrix(coef(GN))
```

```{python}
C.T[10][1:]
```

```{python}
GN.coefs_[10]
```

```{python}
assert np.allclose(C.T[10][1:], GN.coefs_[10], rtol=1e-4, atol=1e-4)
```

# Use one of the C++ paths

## Without CV

```{python}
GN2 = GaussNet(response_col='response',
               offset_col='offset',
               weight_col='weight',
               ).fit(X, Df)

```

```{r magic_args="-o C"}
GN2 = glmnet(X, R, weights=W, offset=O)
C = as.matrix(coef(GN2))
```

```{python}
assert np.allclose(C[1:].T, GN2.coefs_)
```

```{python}
assert np.allclose(C[0], GN2.intercepts_)
```

## Now with CV, first no weights

```{python}
GN3 = GaussNet(response_col='response',
               offset_col='offset',
               ).fit(X, Df)

```

Capture the fold ids

```{python}
cv = KFold(5, random_state=0, shuffle=True)
foldid = np.empty(n)
for i, (train, test) in enumerate(cv.split(np.arange(n))):
    foldid[test] = i+1
```

## With an offset using `fraction`

```{python}
GN3.cross_validation_path(X, Df, cv=cv, alignment='fraction');
```

```{python}
GN3.plot_cross_validation(score='Mean Squared Error', xvar='lambda')
```

```{r magic_args="-i foldid -o CVM,CVSD"}
foldid = as.integer(foldid)
W = as.numeric(W)
O = as.numeric(O)
R = as.numeric(R)
GCV = cv.glmnet(X,
                R, 
                offset=O,
                foldid=foldid,
		alignment="fraction",
		grouped=TRUE)
plot(GCV)
CVM = GCV$cvm
CVSD = GCV$cvsd
```

```{python}
assert np.allclose(GN3.cv_scores_['Mean Squared Error'], CVM)
assert np.allclose(GN3.cv_scores_['SD(Mean Squared Error)'], CVSD) 
```

## With an offset using `lambda`

```{python}
GN3 = GaussNet(response_col='response',
               offset_col='offset',
               ).fit(X, Df)
GN3.cross_validation_path(X, Df, cv=cv, alignment='lambda');
```

```{python}
GN3.plot_cross_validation(score='Mean Squared Error', xvar='lambda')
```

```{r magic_args="-i foldid -o CVM,CVSD"}
foldid = as.integer(foldid)
W = as.numeric(W)
O = as.numeric(O)
R = as.numeric(R)
GCV = cv.glmnet(X,
                R, 
                offset=O,
                foldid=foldid,
		alignment="lambda",
		grouped=TRUE)
plot(GCV)
CVM = GCV$cvm
CVSD = GCV$cvsd
```

```{python}
assert np.allclose(GN3.cv_scores_['Mean Squared Error'], CVM)
assert np.allclose(GN3.cv_scores_['SD(Mean Squared Error)'], CVSD) 
```

## With an offset and weight using `fraction`

```{python}
GN4 = GaussNet(response_col='response',
               offset_col='offset',
	           weight_col='weight',
               ).fit(X, Df)
GN4.cross_validation_path(X, Df, cv=cv, alignment='fraction');
```

```{python}
debug
```

```{python}
GN4.plot_cross_validation(score='Mean Squared Error', xvar='lambda')
```

```{r magic_args="-i foldid -o CVM,CVSD"}
foldid = as.integer(foldid)
W = as.numeric(W)
O = as.numeric(O)
R = as.numeric(R)
GCV = cv.glmnet(X,
                R, 
                offset=O,
                weights=W,
                foldid=foldid,
		        alignment="fraction",
		        grouped=TRUE)
plot(GCV)
CVM = GCV$cvm
CVSD = GCV$cvsd
```

```{python}
assert np.allclose(GN4.cv_scores_['Mean Squared Error'], CVM)
assert np.allclose(GN4.cv_scores_['SD(Mean Squared Error)'], CVSD) 
```

## With an offset and weight using `lambda`

```{python}
GN4 = GaussNet(response_col='response',
               offset_col='offset',
               weight_col='weight',
               ).fit(X, Df)
GN4.cross_validation_path(X, Df, cv=cv, alignment='lambda');
```

```{python}
GN4.plot_cross_validation(score='Mean Squared Error', xvar='lambda')
```

```{r magic_args="-i foldid -o CVM,CVSD"}
foldid = as.integer(foldid)
W = as.numeric(W)
O = as.numeric(O)
R = as.numeric(R)
GCV = cv.glmnet(X,
                R, 
                offset=O,
                weights=W,
                foldid=foldid,
        		alignment="lambda",
        		grouped=TRUE)
plot(GCV)
CVM = GCV$cvm
CVSD = GCV$cvsd
```

```{python}
assert np.allclose(GN4.cv_scores_['Mean Squared Error'], CVM)
assert np.allclose(GN4.cv_scores_['SD(Mean Squared Error)'], CVSD) 
```



```{python}

```
